{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian approach to inverse problems\n",
    "##### Data Driven Fluid Mechanics and Machine Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import several useful modules\n",
    "import numpy as np \n",
    "import random\n",
    "import matplotlib.pyplot as plt \n",
    "from scipy.integrate import simps\n",
    "from scipy.optimize import least_squares\n",
    "import functools\n",
    "from scipy import linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we need to define the model as a function of the variable input x and parameters w\n",
    "def increasing_sinus_model(x, w):\n",
    "    y = w[0]*np.sin(w[1]*x*(np.pi/2)) + w[1]*x \n",
    "    \n",
    "    return y \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate synthetic data \n",
    "We need data to perform the inference. We generate synthetic data using the probabilistic model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We fix the parameter values in order to generate the data\n",
    "w0 = 5.; w1 = 3.\n",
    "dim = 2 # Dimension of the problem \n",
    "\n",
    "# data x-field \n",
    "nx_data = 200 # can be 20, 50, 100, 400, etc \n",
    "x_data = np.linspace(0, 10, nx_data) \n",
    "\n",
    "# Generate Gaussian perturbations with standard deviation sigma_y \n",
    "sigma_y = 1.5\n",
    "rn = [(random.gauss(0, sigma_y)) for i in range(nx_data)]\n",
    "\n",
    "# You could check that your data looks normal by checking the histogram  \n",
    "plt.figure(0)\n",
    "plt.hist(rn, 20)\n",
    "\n",
    "# Generate the baseline solution from the model at x_data\n",
    "y_nom = increasing_sinus_model(x_data, [w0, w1])\n",
    "\n",
    "# Perturb the data \n",
    "y_noisy = y_nom + rn\n",
    "\n",
    "# Plot the data and the data generating functon \n",
    "# For vizualisation, the x-field for the model is refined\n",
    "nx_model = 400\n",
    "x_model = np.linspace(0, 10, nx_model) \n",
    "y_nom = increasing_sinus_model(x_model, [w0, w1])\n",
    "\n",
    "# Plot data\n",
    "plt.figure(1)\n",
    "plt.plot(x_data, y_noisy, 'o', markersize=2, label=\"Data\")\n",
    "plt.plot(x_model, y_nom, label=\"Data-generating function\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the MLE by solving the least-square problem. You can use your \n",
    "# favorite optimization algorithm. Here, we use the Levenberg-Marquardt \n",
    "# algorithm as implemented in MINPACK. Doesnâ€™t handle bounds and sparse Jacobians. \n",
    "# Usually the most efficient method for small unconstrained problems.\n",
    "lsq_fun = lambda w: (increasing_sinus_model(x_data, w)-y_noisy)\n",
    "res = least_squares(lsq_fun, [w0, w1], method=\"lm\")\n",
    "w_MLE = res.x\n",
    "print(\"Value of the MLE = {}\".format(w_MLE))\n",
    "\n",
    "\n",
    "# x-field for the model can have a more refine discretization for plotting the result \n",
    "nx_model = 400\n",
    "x_model = np.linspace(0,10,nx_model) \n",
    "\n",
    "# The MLE will be closed to the data generating function, but it varies every time new random data are generated\n",
    "y_MLE = increasing_sinus_model(x_model, w_MLE) \n",
    "\n",
    "# Plot data \n",
    "plt.figure(2)\n",
    "plt.plot(x_data, y_noisy, 'o', markersize=2, label=\"Data\")\n",
    "plt.plot(x_model, y_nom, label=\"Data-generating function\")\n",
    "plt.plot(x_model, y_MLE, '--', markersize=2, label=\"MLE\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Parameter inference\n",
    "Now that we have the data, we can build the posterior from Bayes' formula "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define likelihood function.\n",
    "# We will use the gaussian pdf (check lecture 6) \n",
    "def fun_LL(w):\n",
    "    return np.exp(-1/2/sigma_y**2*np.sum((y_noisy - increasing_sinus_model(x_data, w))**2))\n",
    "    \n",
    "# Prior function is uniform and is just a constant \n",
    "\n",
    "\n",
    "# Because the problem is 2D, we can easily compute the posterior by screening the values of the parameters \n",
    "n_point_screening = 400\n",
    "vec_param_i = np.linspace(3.5, 6.5, n_point_screening)\n",
    "vec_param_j = np.linspace(2.9, 3.10, n_point_screening)\n",
    "\n",
    "f_post = np.zeros([vec_param_i.size, vec_param_j.size])\n",
    "for i, param_i in np.ndenumerate(vec_param_i): \n",
    "    for j, param_j in np.ndenumerate(vec_param_j): \n",
    "        c_param = np.array([param_i, param_j])\n",
    "        f_post[i,j] = fun_LL(c_param) \n",
    "        \n",
    "# The posterior needs to be normalized. Integration can be performed in both directions     \n",
    "f_post_w0 = simps(f_post, vec_param_j) # Integrate over f_w1\n",
    "f_post_w1 = simps(f_post, vec_param_i) #Integrate over f_w0\n",
    "norm_f_post = simps(f_post_w0, vec_param_i) # Integrate over f_w0\n",
    "norm_f_post_2 = simps(f_post_w1, vec_param_j) # Integrate over f_w1\n",
    "print(norm_f_post, norm_f_post_2) # They should be the same \n",
    "f_post /= norm_f_post_2 # or norm_f_post_2\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.contour(vec_param_j, vec_param_i, f_post, 25, cmap='RdGy')\n",
    "# We can also plot on the graph the MLE ! \n",
    "plt.plot(w_MLE[1], w_MLE[0], 'o')\n",
    "plt.ylim([4.5, 5.5]) # xlim and ylim might need adjustement \n",
    "plt.xlim([2.99, 3.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metropolis-Hastings algorithm\n",
    "We could obtain the posterior distribution by screening the parameter values. However, here we want to sample from it. So let's build the MCMC algorithm, in particular the random-walk Metropolis-Hastings.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MCMC method parameter \n",
    "n_it = int(1e4)\n",
    "proposal_cov = np.array([[0.05,0.], [0.,0.0001]])\n",
    "w_n = np.zeros((n_it+1, dim)) # np array to store the sample values\n",
    "\n",
    "# Initialise \n",
    "R = linalg.cholesky(proposal_cov)\n",
    "w_n[0,:] = w_MLE\n",
    "\n",
    "# MCMC lopp \n",
    "for n in range(n_it): \n",
    "\n",
    "    # 1. Sample from the multivariate Gaussian \n",
    "    # First, we generate the normal centering gaussian \n",
    "    mv_norm = np.zeros(dim)\n",
    "    for j in range(dim):\n",
    "        mv_norm[j] = random.gauss(0, 1)  \n",
    "\n",
    "    w_star = w_n[n, :] + np.transpose(np.matmul(R, np.transpose(mv_norm)))\n",
    "\n",
    "    # 2. Compute posterior value at the proposed sample \n",
    "    r = fun_LL(w_star)/fun_LL(w_n[n,:])\n",
    "    \n",
    "    # 3. Compute acceptance probability \n",
    "    alpha = min(1, r)\n",
    "    \n",
    "    # 4. Accept-reject\n",
    "    if random.random() < alpha:  # Accepted\n",
    "        w_n[n+1,:] = w_star\n",
    "    else:  # Rejected, current val remains the same\n",
    "        w_n[n+1,:] = w_n[n,:]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "      \n",
    "plt.figure(4)\n",
    "plt.plot(w_n[:, 0])\n",
    "plt.ylabel(\"w0\")\n",
    "plt.xlabel(\"n\")\n",
    "\n",
    "plt.figure(5)\n",
    "plt.plot(w_n[:, 1])\n",
    "plt.ylabel(\"w1\")\n",
    "plt.xlabel(\"n\")\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Contour plot with a few MCMC samples\n",
    "plt.figure(6)\n",
    "plt.contour(vec_param_j, vec_param_i, f_post, 25, cmap='RdGy')\n",
    "plt.plot(w_n[::50, 1], w_n[::50, 0], 'o')\n",
    "plt.ylim([4.5, 5.5]) # xlim and ylim might need adjustement \n",
    "plt.xlim([2.99, 3.01])\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
